# インターネットアクセス

## Requests
「Requests」とはWebサイトにアクセスし、データを操作できるHTTPライブラリのこと。

#### ▼ Requestsのインストールコマンド
```
pip install requests
```

#### ▼ Requestsの書き方
- 各HTTPメソッド
    ```
    import requests
    
    # GETメソッド
    requests.get('URL')
    # POSTメソッド
    requests.post('URL')
    # PUTメソッド
    requests.put('URL')
    # DELETEメソッド
    requests.delete('URL')
    ```

- getメソッドの引数
    ```
    response = requests.get(URL, その他任意の引数)
    ```

    | 引数    | 必須/任意 | 説明                                   |
    | :------ | :-------: | :------------------------------------- |
    | URL     |   必須    | 読み込み対象のURL                      |
    | headers |   任意    | ヘッダとして送信する内容を辞書で指定   |
    | timeout |   任意    | リクエストのタイムアウト時間           |
    | params  |   任意    | URLのクエリパラメータを辞書で指定      |
    | cookies |   任意    | クッキーとして送信する内容を辞書で指定 |

- getメソッドの戻り値
    | 属性        | 説明                       |
    | :---------- | :------------------------- |
    | status_code | ステータスコード           |
    | headers     | レスポンスヘッダの内容     |
    | content     | レスポンスのバイナリデータ |
    | text        | レスポンスの内容           |
    | encoding    | エンコーディング           |
    | cookies     | クッキーの内容             |

- getメソッドを使ってデータを取得してみる
    - text（HTML）データ
        ```
        import requests

        url = 'https://news.yahoo.co.jp'
        response = requests.get(url)
        print(response.text[:500]) # 先頭の500文字のみ指定
        ```
        実行結果
        ```
        <!DOCTYPE html><html lang="ja"><head><title>Yahoo!ニュース</title><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="viewport" content="width=1010"/><meta http-equiv="Content-Style-Type" content="text/css"/><meta http-equiv="Content-Script-Type" content="text/javascript"/><meta name="description" content="Yahoo!ニュースは、新聞・通信社が配信するニュースのほか、映像、雑誌や個人の書き手が執筆する記事など多種多様なニュースを掲載しています。"/><meta name="robots" content="noarchive, max-image-preview:large"/><meta name="format-detection"
        ```

    - ステータスコード
        ```
        import requests

        url = 'https://news.yahoo.co.jp'
        response = requests.get(url)
        print(response.status_code)
        ```
        実行結果
        ```
        200
        ```
- WikipediaのAPIを使ってデータを取得してみる
    ```
    import requests
    import sys

    title = input('何を検索しますか? >') 
    url = 'https://ja.wikipedia.org/w/api.php'
    api_params1 = {
                    'action': 'query',
                    'titles': title,
                    'prop': 'categories',
                    'format': 'json'
                }

    api_params2 = {
                    'action': 'query',
                    'titles': title,
                    'prop': 'revisions',
                    'rvprop': 'content',
                    'format': 'xmlfm'
                }
    categories = requests.get(url, params=api_params1).json()
    page_id = categories['query']['pages']
    if '-1' in page_id:
        print('該当するページがありません')
        sys.exit()
        
    else:
        id = list(page_id.keys())
        if 'categories' in categories['query']['pages'][id[0]]:
            categories = categories['query']['pages'][id[0]]['categories']
            for t in categories:
                print(t['title'])
        else:
            print('保存できるページを検索できませんでした')
            sys.exit()

    admit = input('検索結果を保存しますか?(yes) >') 
    if admit == 'yes':
        data = requests.get(url, params=api_params2)
        with open(title + '.html', 'w', encoding = 'utf_8') as f:
            f.write(data.text)
    else:
        print('プログラムを終了します')
        sys.exit()
    ```
    実行結果
    ```
    何を検索しますか? >東京都 # 「東京都」と入力
    Category:15世紀の日本の設立
    Category:BNFdata識別子が指定されている記事
    Category:BNF識別子が指定されている記事
    Category:CINII識別子が指定されている記事
    Category:CRID識別子が指定されている記事
    Category:FAST識別子が指定されている記事
    Category:GND識別子が指定されている記事
    Category:Geoマイクロフォーマットがある記事
    Category:ISBNマジックリンクを使用しているページ
    Category:ISNI識別子が指定されている記事
    検索結果を保存しますか?(yes) >yes # yesを入力してhtml形式でファイルを保存
    ```
